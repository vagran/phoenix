====== Phoenix virtual memory subsystem ======

VM subsystem is one of the most important subsystems in the Phoenix OS which reflects whole the system design.

===== Physical memory mapping =====

There are two commonly used approaches for mapping the physical memory in the OS:
  - Straight-forward simple way -- map all available physical memory persistently to continuous virtual address space region and use it to access physical pages. The mapping is visible to the kernel only, user space maps are constructed separately. Linux is utilizing this approach.
  - More advanced and elegant way -- map LAT((Linear Address Translation)) tables hierarchy using recursive mapping, allocate and map physical pages on demand. BSD-based UNIX systems use this approach.

Both methods have pros and cons, one of them should be selected for Phoenix OS.

==== Persistent PM mapping ====

Pros:

  - Simple physical memory management. It is reflected as virtual address space region.
  - One allocator can be used to allocate kernel dynamical memory. Allocating some range in PM mapping region in the same time allocates physical memory range.
  - All dynamical memory ranges allocated by the kernel are automatically mapped to **continuous** physical memory pages. It is often required for memory regions used in DMA transactions.
  - Any physical memory page is instantly accessible by the kernel. It is never required to make some new persistent or temporary mapping for a page.

Cons:
  - Overhead of memory usage by LAT tables which are required to map all physical memory. Often the kernel does not need access to the most of physical pages (e.g. these which contain user space processes data) but they are still mapped to the PM mapping region and consume physical pages for LAT tables. However this drawback can be considered not so important because required overhead is ''1/N + 1/N<sup>2</sup> + 1/N<sup>3</sup> + ... + 1/N<sup>M</sup>'' bytes, where N is number of entries per LAT table, M -- number of tables in the hierarchy. For example, x86_64 architecture will have memory consumption overhead about 0.2%. It is very small, especially accounting that there will be many times bigger overhead from page descriptors array.
  - On 32-bits architectures virtual address space is too small to hold the whole PM mapping. On Linux it became a problem when typical RAM size became greater than 768MB. The workaround was mapping directly only 768MB of PM, and the rest is mapped dynamically on demand. The cost for this workaround is performance degradation when accessing dynamically mapped PM. This con can be accounted with low importance level since Phoenix is designed for 64-bits architectures.
  - PTE lookup for a given virtual address must be done by software which should traverse all LAT tables hierarchy. However in general case with recursive mapping all tables hierarchy also should be traversed and entry presence should be checked at each level. PTE can be accessed instantly only if it is known that all upper level tables are present (e.g. if there are no page table (PDE is not present), accessing to PTE will cause page fault).

==== Recursive mapping ====

Pros:
  - Reduced memory consumption by LAT tables because of two factors:
    * Only really used PM is mapped to the kernel address space. 
    * Mapping for LAT tables themselves is done automatically by recursive LAT root table entries. It ensures that all LAT tables map themselves with zero additional tables required.
  - Easier PTE access by a given virtual address. This is ensured by recursive LAT tables mapping so the PTE lookup is assisted by CPU hardware LAT capabilities (on the architectures where LAT tables traversal is done by hardware).


Cons:
  - Only current address space PTEs can be accessed using recursive LAT mapping. If other address space mapping must be modified, it is needed to have one or several alternative address spaces recursively mapped in the current address space. However other address spaces modifications are not often required.
  - In order to access some arbitrary physical page content, a temporal mapping of that page must be created and released after the access.
  - Special measures must be taken in order to be able to allocate continuous physical memory (e.g. some separate allocator for physical address space).

==== Conclusion ====

Both methods also require additional array of pages descriptors which store additional page attributes required by the system (reference counters, mapping associations, lists/trees entries, swapping-related information etc.).

Both methods can be partially combined to have their benefits merged. It is possible to have persistent PM mapping with recursive mapping for LAT tables at the same time. It will give some benefits to the first approach with almost zero cost.

The persistent physical memory mapping is selected for Phoenix because of these factors:
  - The system is designed for 64-bits (and greater) architectures only. So whole the RAM space can be easily mapped to continuous virtual address space region.
  - The system has abstraction layer for LAT functionality, which in most cases will require LAT tables entries checking on all levels. It will anyway requires traversing all tables during PTE lookup so the translation itself can be done in software almost without additional cost. Recursive mapping benefit for PTE lookup does not give any advantages with this approach.
  - Phoenix specific behavior of VM subsystem requires many operations between different virtual address spaces. The second approach is not well-suitable for working with more than one (current) virtual address space while the first one does not have any limitations on manipulating LAT tables from different virtual address spaces.

A single virtual addresses range is selected to be able to reflect physical address space range starting from the lowest available RAM physical address up to the highest one. All available chunks are mapped there (there could be many holes which are occupied by firmware or hardware). Physical memory allocator is placed on top of this mapping. The allocator is aware of all available regions. It can allocate separate physical pages which can be mapped e.g. to user space maps, or it can allocate bigger continuous regions which can be used directly by the kernel.

===== Virtual address space distribution =====

The lower 2GB is a space for all executable binaries linking. So all executables are built with an assumption that all sections and symbols will be located in lower 2GB of VM space. This is required to allow compiler to use 32 bits offsets when referencing program symbols. Each container has its own private virtual address space in the lowest 2GB where its executable image and all required shared libraries are mapped.

The kernel image is mapped to the top of the lowest 2GB and stays there constantly for all processes. Kernel temporal boot stack is allocated in the kernel BSS area. Later it is reallocated in the kernel dynamical memory for each CPU. Above the BSS area the temporal kernel heap is located. It is linearly growing and used only on the early booting stage until the kernel memory manager is initialized. At the top of kernel linkage space **kernel gate area** is located. At this location the kernel can place entry points which can be executed by user space code in order to enter kernel mode.

''SYS_DATA_SIZE'' bytes above linkage space is the system data space -- the kernel can allocate virtual address space ranges there, for example, to use for memory mapped devices. The region is mapped for system-only access.

The space above is the global data space. This region is used for allocating all containers memory. When a container is activated it has only its own pages mapped in the global address space (and shared pages, see map entries description). This space is divided in two parts -- small bootstrap area and the rest space for normal memory. Bootstrap area is used for system bootstrapping -- it is used for containers created when system is booting. Each of these areas are in turn divided on other two parts -- resident and normal areas. Memory allocated in a resident area consists from wired pages and is used by resident containers (e.g. devices drivers). The resident area size should be reasonably small because there is no meaning for having this area greater than size of available physical memory. Normal area is used all the rest allocations and pages in this area are normally involved in paging.

The top of virtual address space contains persistent physical memory mapping. The kernel can access to any physical memory page via this mapping. The kernel can allocate physical memory ranges from this region. It can be either single pages to map into user space or continuous physical memory ranges to use, for example, for DMA transactions. Also this region allocated ranges can be used by the kernel directly so it becomes the kernel dynamical memory pool after kernel memory manager is initialized. This area has system-only access and used by physical maps management code in the kernel.

^  Phoenix virtual memory model  ^
|{{:athena:product:phoenix:components:memory_model.png|Phoenix virtual memory model}}|


===== VM model main objects =====

The following objects are fundamental bricks of Phoenix VM subsystem model.

==== Page ====

Page object represents physical memory page. In Phoenix it always has 4KB size.

Page can be **wired** -- locked into physical memory and will not be paged out until wiring counter is not zero.

During system start-up all available physical memory is revised and page descriptors array is created. Array size correspond to range of managed pages. In order to eliminate additional pages wasting for the array itself, the physical memory for the array is taken from the top of available physical memory.

^ Mapping elements sizes ^^^
| PTE | 1 page | 4KB |
| PDE | 512 pages | 2MB |
| PDPTE | 256K pages | 1GB |
| PLM4E | 128M pages | 512GB |

Access control is done by PTEs flags. All other entities have maximal permissions (excluding recursive mapping entities which have system-only access).

==== Map ====

Map describes either entire virtual address space or some region of virtual address space (submap). Maps could be organized to hierarchy so that any map could have any numbers of submaps. Submaps can be used, for example, to limit allocation regions. Activating some map means switching entire virtual address space to a new one. A map has a buddy allocator which allocates space in the map. Space can be allocated at fixed position or selected by an allocator. For each allocated entry a new entity is created -- **map entry** which holds information about that region usage.

Map stores only machine-independent part of mapping information. A map has pointer to associated **physical map** object which stores machine-dependent information such as physical address translation tables.

==== Physical map ====

Physical map represents machine-dependent part of virtual address space. Typically it contains linear address translation (LAT) tables and have methods to activate address space in hardware (e.g. write LAT tables root pointer to a corresponding CPU register).

==== Map entry ====

Map entry represents some region in a map. It could be either reserved space, memory mapped device, or representing some region of some container memory. It describes how the content of corresponding virtual address space region should be processed, for example, in case of page faults. It also describes properties of the region mapping, such as protection, caching, paging etc.

Map entries can be organized in hierarchical structures. One map entry could reference another entry as a source of data pages. The referenced entry can belong to the same or to another map. This allows mapping some memory region of one container to another container (or to the same one but to different address and optionally with different protection) saving physical memory pages. Copy-on-write (COW) protection can be used for such mapping. The following relations persist between such entries:
  * Backing entry -- entry which is a source of data pages for a given entry.
  * Shadow entry -- entry which is a child entry for a given entry. It "shadows" some part of backing entry pages -- when it is accessed, pages which exists in shadow entry are taken from it, the rest pages are taken from backing entry. If shadow entry has COW protection and there is a write access to a page which does not exist in shadow entry, the corresponding page is copied from backing entry and inserted in shadow entry where its content is modified.

A shadow entry can map either entire backing entry or only part of it from arbitrary page-aligned offset. Its projection should not cross backing entry boundaries.

Each page in map entry can have the following state:
  * Non-existent pages -- the pages are not yet allocate. Usually pages in an entry are allocated on the first access.
  * Allocated pages -- in typical cases pages of an entry are allocated on the first access. While they were not accessed they are not allocated -- i.e. does not consume nor physical pages nor external storage space. Pages can be optionally zeroed during allocation (practically taken from pool of zeroed physical pages).
  * Resident pages -- allocated pages which are currently resident in physical memory. An entry may have not all pages present in physical memory. Some of them can be swapped to external storage. They are loaded to the memory on an access.

===== Memory management =====

As it was described above the kernel has all physical memory mapped persistently to dedicated virtual addresses range. The kernel physical memory allocator is running on top of this range. It is responsible for allocating either separate physical pages or continuous ranges of physical pages. Separate pages are allocated when it is needed to create some new mapping in a virtual address space. Physical pages are allocated one by one and entered into (continuous) virtual addresses range. Such purpose does not require that physical pages must be in continuous range. Continuous ranges of physical pages are allocated when the kernel requires more dynamical memory (heap). The kernel can use the allocated pages directly via their persistent virtual addresses. It also can pass the allocated block physical address to a device driver to use this block, for example, for DMA transactions. If some device driver requests physical memory, it receives the allocated block physical address as well as virtual address of the private mapping created for this driver container. Device drivers must take into account that these mappings and physical addresses are valid only until the system reboot. After a system is rebooted drivers will receive notifications and should reinitialize devices and request new physical memory if necessary.

The kernel uses scalable slab allocator for the kernel own dynamical memory allocations. The slab allocator uses the kernel physical memory allocator as a back-end to request more memory. It requests the memory in large blocks (slabs) which are divided into smaller blocks of common sizes and returned to the kernel.

The global memory manager running in the user space uses the kernel memory manager via the kernel provided interfaces. In this tandem the kernel is managing physical pages and physical maps. Corresponding interfaces are provided for allocating and freeing physical pages, entering them to physical maps with required permissions. Memory Manager component has permissions for using these interfaces. It is responsible for managing maps and map entries for all containers and threads.

Pagers also have access to the kernel paging interfaces.


===== Memory allocations =====

Buddy allocator is used for allocating space in the global address space. It is used for allocating large blocks (staring from one page and greater) which are passed to scalable slab allocator which splits these large blocks to smaller well fitted units. This approach ensures optimal memory utilization.

Memory allocation API does not provide any functions to the front-end code other than C++ "''new''" and "''delete''" operators which are the only way to allocate and free dynamical memory.

==== Buddy allocator ====

The universal buddy allocator (see [[wp>Buddy_memory_allocation|article about buddy allocators]]) is implemented in Phoenix for managing various address ranges. The allocator is abstracted from underlying resource type, i.e. it does not know what kind of resource is allocated -- is it physical or virtual memory, some ID space or anything else. It operates with address ranges only. The allocator must provide the following functionality:
  * Allocate a block of specified size where possible.
  * Allocate a block of specified size at specified location if possible.
  * Free block at specified location.
  * Find block size, location (start address) and status (allocated/free) by specified arbitrary address which  belongs to this block.
Note that the allocator is capable for allocations and freeings only -- it does not provide any way to associate some client meta-information with allocated blocks. If such meta-information is required, the client should implement some mapping between block address and its meta-information block (e.g. using binary search trees).

The buddy allocator is implemented using blocks bitmaps and free blocks cache. When the allocator is initialized it allocates several internal structures -- a set of block bitmaps and a pool of cached free blocks. The set of bitmaps is a map of currently allocated blocks. Each bitmap corresponds to one order with which the allocator operates. Each set bit in a bitmap corresponds to an allocated block of that order.

Free blocks cache is a number of trees (one tree for each order) which store some amount of free blocks of each order. Trees are used because when a block is freed, its buddy can be coalesced if it is free. In such case it must be removed from free blocks cache of one order and placed to another one. In case lists are used instead of trees for free blocks cache then linear search should be used in order to find coalesced block and delete it from cache.
